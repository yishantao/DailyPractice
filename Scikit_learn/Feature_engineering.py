# -*-coding:utf-8 -*-

"""This module is used for feature engineering"""

from sklearn.datasets import load_iris

# 导入IRIS数据集
iris = load_iris()

# # 特征矩阵
# print(iris.data)
# # 目标向量
# print(iris.target)

# from sklearn.preprocessing import StandardScaler
#
# # 标准化，返回值为标准化后的数据
# print(StandardScaler().fit_transform(iris.data))

# from sklearn.preprocessing import MinMaxScaler
#
# # 区间缩放，返回值为缩放到[0, 1]区间的数据
# print(MinMaxScaler().fit_transform(iris.data))

# from sklearn.preprocessing import Normalizer
#
# # 归一化，返回值为归一化后的数据
# print(Normalizer().fit_transform(iris.data))

# from sklearn.preprocessing import Binarizer
#
# # 二值化，阈值设置为3，返回值为二值化后的数据
# print(Binarizer(threshold=3).fit_transform(iris.data))

# from sklearn.preprocessing import OneHotEncoder
#
# # 哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据
# print(OneHotEncoder().fit_transform(iris.target.reshape((-1, 1))))

# from numpy import vstack, array, nan
# from sklearn.preprocessing import Imputer
#
# # 缺失值计算，返回值为计算缺失值后的数据
# # 参数missing_value为缺失值的表示形式，默认为NaN
# # 参数strategy为缺失值填充方式，默认为mean（均值）
# print(Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))))


# from sklearn.preprocessing import PolynomialFeatures
#
# # 多项式转换
# # 参数degree为度，默认值为2
# print(PolynomialFeatures().fit_transform(iris.data))


# from numpy import log1p
# from sklearn.preprocessing import FunctionTransformer
#
# # 自定义转换函数为对数函数的数据变换
# # 第一个参数是单变元函数
# print(FunctionTransformer(log1p).fit_transform(iris.data))


# from sklearn.feature_selection import VarianceThreshold
#
# # 方差选择法，返回值为特征选择后的数据
# # 参数threshold为方差的阈值
# print(VarianceThreshold(threshold=3).fit_transform(iris.data))


# from sklearn.feature_selection import SelectKBest
# from scipy.stats import pearsonr
# from numpy import array
#
# # 选择K个最好的特征，返回选择特征后的数据
# # 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
# # 参数k为选择的特征个数
# print(SelectKBest(lambda X, Y: array(list(map(lambda x: pearsonr(x, Y), X.T))).T[0], k=2).fit_transform(iris.data, iris.target))


# from sklearn.feature_selection import SelectKBest
# from sklearn.feature_selection import chi2
#
# # 选择K个最好的特征，返回选择特征后的数据
# print(SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target))


# from sklearn.feature_selection import SelectKBest
# from minepy import MINE
# from numpy import array
#
# # 由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
# def mic(x, y):
#     m = MINE()
#     m.compute_score(x, y)
#     return (m.mic(), 0.5)
#
#
# # 选择K个最好的特征，返回特征选择后的数据
# SelectKBest(lambda X, Y: array(list(map(lambda x: mic(x, Y), X.T))).T[0], k=2).fit_transform(iris.data, iris.target)


# from sklearn.feature_selection import RFE
# from sklearn.linear_model import LogisticRegression
#
# # 递归特征消除法，返回特征选择后的数据
# # 参数estimator为基模型
# # 参数n_features_to_select为选择的特征个数
# print(RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target))


# from sklearn.feature_selection import SelectFromModel
# from sklearn.linear_model import LogisticRegression
#
# # 带L1惩罚项的逻辑回归作为基模型的特征选择
# print(SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target))


# from sklearn.linear_model import LogisticRegression
#
#
# class LR(LogisticRegression):
#     def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0,
#                  fit_intercept=True, intercept_scaling=1, class_weight=None,
#                  random_state=None, solver='liblinear', max_iter=100,
#                  multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):
#
#         # 权值相近的阈值
#         self.threshold = threshold
#         LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C,
#                                     fit_intercept=fit_intercept, intercept_scaling=intercept_scaling,
#                                     class_weight=class_weight,
#                                     random_state=random_state, solver=solver, max_iter=max_iter,
#                                     multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)
#         # 使用同样的参数创建L2逻辑回归
#         self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept,
#                                      intercept_scaling=intercept_scaling, class_weight=class_weight,
#                                      random_state=random_state, solver=solver, max_iter=max_iter,
#                                      multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)
#
#     def fit(self, X, y, sample_weight=None):
#         # 训练L1逻辑回归
#         super(LR, self).fit(X, y, sample_weight=sample_weight)
#         self.coef_old_ = self.coef_.copy()
#         # 训练L2逻辑回归
#         self.l2.fit(X, y, sample_weight=sample_weight)
#
#         cntOfRow, cntOfCol = self.coef_.shape
#         # 权值系数矩阵的行数对应目标值的种类数目
#         for i in range(cntOfRow):
#             for j in range(cntOfCol):
#                 coef = self.coef_[i][j]
#                 # L1逻辑回归的权值系数不为0
#                 if coef != 0:
#                     idx = [j]
#                     # 对应在L2逻辑回归中的权值系数
#                     coef1 = self.l2.coef_[i][j]
#                     for k in range(cntOfCol):
#                         coef2 = self.l2.coef_[i][k]
#                         # 在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0
#                         if abs(coef1 - coef2) < self.threshold and j != k and self.coef_[i][k] == 0:
#                             idx.append(k)
#                     # 计算这一类特征的权值系数均值
#                     mean = coef / len(idx)
#                     self.coef_[i][idx] = mean
#         return self
#
#
# from sklearn.feature_selection import SelectFromModel
#
# # 带L1和L2惩罚项的逻辑回归作为基模型的特征选择
# # 参数threshold为权值系数之差的阈值
# print(SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target))


# from sklearn.feature_selection import SelectFromModel
# from sklearn.ensemble import GradientBoostingClassifier
#
# # GBDT作为基模型的特征选择
# print(SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target))


# from sklearn.decomposition import PCA
#
# # 主成分分析法，返回降维后的数据
# # 参数n_components为主成分数目
# print(PCA(n_components=2).fit_transform(iris.data))


# from sklearn.lda import LDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# 线性判别分析法，返回降维后的数据
# 参数n_components为降维后的维数
print(LDA(n_components=2).fit_transform(iris.data, iris.target))
